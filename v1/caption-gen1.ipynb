{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import re\n",
    "from pickle import dump, load\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm().pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory name of the text data\n",
    "dataset_text_dirname = \"Flickr8k_text\"\n",
    "# Directory name of the image data\n",
    "dataset_image_dirname = \"Flickr8k_Dataset/Flicker8k_Dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility\n",
    "\n",
    "def load_file(filepath: str) -> str:\n",
    "    \"\"\"\n",
    "    @param filepath path of the file to be loaded\n",
    "    @return the contents of the file\n",
    "    \"\"\"\n",
    "    file = open(filepath, \"r\")\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "def get_img_ids(dataset_text_filepath: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    @param dataset_text_filepath path of the file containing image ids\n",
    "    @return list of the image ids\n",
    "    \"\"\"\n",
    "    text = load_file(dataset_text_filepath)\n",
    "    img_ids = text.split()\n",
    "    return img_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_name_filename = \"Flickr_8k.trainImages.txt\"\n",
    "img_name_path = dataset_text_dirname + \"/\" + img_name_filename\n",
    "img_ids = get_img_ids(img_name_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process text dataset into descriptions\n",
    "\n",
    "def get_descriptions(dataset_text_filepath: str, img_ids: list[str] = [], cleaned: bool = False) -> dict[str, list[str]]:\n",
    "    \"\"\"\n",
    "    Load the descriptions from the text dataset into a dictionary\n",
    "\n",
    "    @param dataset_text_filepath path of the file containing image ids and captions\n",
    "        >>> file contents\n",
    "        1000268201_693b08cb0e.jpg#0\\tA child in a pink dress is climbing up a set of stairs in an entry way .\\n\n",
    "        1000268201_693b08cb0e.jpg#1\\tA girl going into a wooden building .\\n\n",
    "        ...\n",
    "    @param img_ids list of image ids the dictionary should have. This can add and remove items from the dictionary. If empty return all\n",
    "    @param cleaned True if the descriptions to be read have already been processed and cleaned\n",
    "    @return dictionary of descriptions (key: image id -> value: array of image captions)\n",
    "    \"\"\"\n",
    "    text = load_file(dataset_text_filepath)\n",
    "    entries = text.split(\"\\n\")\n",
    "\n",
    "    descriptions = {}\n",
    "    for entry in entries:\n",
    "        if entry == \"\":\n",
    "            continue\n",
    "        img_id, caption = entry.split(\"\\t\")\n",
    "        if not cleaned:\n",
    "            # Strip numbers off id (ie 1000268201_693b08cb0e.jpg#0 -> 1000268201_693b08cb0e.jpg)\n",
    "            img_id = img_id[:-2]\n",
    "        if img_id not in descriptions:\n",
    "            descriptions[img_id] = [caption]\n",
    "        else:\n",
    "            descriptions[img_id].append(caption)\n",
    "\n",
    "    # Ensure descriptions has inputted image ids\n",
    "    if len(img_ids) > 0:\n",
    "        filtered_descriptions = {}\n",
    "        for img_id in img_ids:\n",
    "            if img_id in descriptions:\n",
    "                filtered_descriptions[img_id] = descriptions[img_id]\n",
    "            else:\n",
    "                filtered_descriptions[img_id] = []\n",
    "        return filtered_descriptions\n",
    "\n",
    "    else:\n",
    "        return descriptions\n",
    "\n",
    "def clean_descriptions(descriptions: dict[str, list[str]]) -> None:\n",
    "    \"\"\"\n",
    "    Clean the entries in the descriptions dictionary in-place.\n",
    "    Convert all letters to lowercase, removes punctuation, removes hanging \"s\" and \"a\"s,\n",
    "    removes words containing numbers, and removes duplicate whitespace\n",
    "\n",
    "    @param descriptions a dictionary (key: image id -> value: array of image captions)\n",
    "    \"\"\"\n",
    "    for img_id, captions in descriptions.items():\n",
    "        for i, caption in enumerate(captions):\n",
    "            # Convert to lowercase\n",
    "            caption = caption.lower()\n",
    "\n",
    "            # Remove punctuation\n",
    "            caption = caption.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "            # Remove hanging \"s\" and \"a\"s\n",
    "            words = caption.split()\n",
    "            caption = \" \".join([word for word in words if word not in [\"a\", \"s\"]])\n",
    "\n",
    "            # Remove words with letters\n",
    "            caption = re.sub(r\"\\w*\\d\\w*\", \"\", caption)\n",
    "\n",
    "            # Remove duplicate whitespace\n",
    "            caption = \" \".join(caption.split())\n",
    "\n",
    "            descriptions[img_id][i] = caption\n",
    "\n",
    "def save_descriptions(filepath: str, descriptions: dict[str, list[str]]) -> None:\n",
    "    \"\"\"\n",
    "    Write the descriptions back to a file\n",
    "\n",
    "    @param filepath the name of the file to write the descriptions to\n",
    "    @param descriptions a dictionary (key: image id -> value: array of image captions)\n",
    "    \"\"\"\n",
    "    lines = list()\n",
    "    for img_id, captions in descriptions.items():\n",
    "        for caption in captions:\n",
    "            description = img_id + \"\\t\" + caption\n",
    "            lines.append(description)\n",
    "\n",
    "    data = \"\\n\".join(lines)\n",
    "    file = open(filepath, \"w\")\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "\n",
    "def dict_to_list(descriptions: dict[str, list[str]]) -> list[str]:\n",
    "    \"\"\"\n",
    "    @param descriptions a dictionary (key: image id -> value: array of image captions)\n",
    "    @return list of all captions in descriptions\n",
    "    \"\"\"\n",
    "    descriptions_list = []\n",
    "    for key, captions in descriptions.items():\n",
    "        for caption in captions:\n",
    "            descriptions_list.append(caption)\n",
    "    return descriptions_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions_filename = \"descriptions.txt\"\n",
    "\n",
    "if os.path.isfile(descriptions_filename):\n",
    "    descriptions = get_descriptions(descriptions_filename, img_ids, True)\n",
    "\n",
    "else:\n",
    "    text_filename = \"Flickr8k.token.txt\"\n",
    "    text_path = dataset_text_dirname + \"/\" + text_filename\n",
    "\n",
    "    descriptions = get_descriptions(text_path)\n",
    "\n",
    "    clean_descriptions(descriptions)\n",
    "    \n",
    "    save_descriptions(descriptions_filename, descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process image dataset into features\n",
    "\n",
    "def extract_features(dataset_img_dirpath: str) -> dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Load the features from the images in the image dataset into a dictionary\n",
    "    \n",
    "    @param dataset_img_dirpath path to the directory containing the images\n",
    "    @return dictionary of features (key: image id -> value: numpy ndarray of features)\n",
    "    \"\"\"\n",
    "    model = tf.keras.applications.xception.Xception(include_top=False, pooling=\"avg\")\n",
    "    \n",
    "    features = {}\n",
    "    images = os.listdir(dataset_img_dirpath)\n",
    "\n",
    "    for img_id in tqdm(images):\n",
    "        img_path = dataset_img_dirpath + \"/\" + img_id\n",
    "        img = Image.open(img_path)\n",
    "        img = img.resize((299, 299))\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        img = img / 127.5\n",
    "        img = img - 1.0\n",
    "        feature = model.predict(img)\n",
    "        features[img_id] = feature\n",
    "\n",
    "    return features\n",
    "\n",
    "def save_features(filepath: str, features: dict[str, np.ndarray]) -> None:\n",
    "    \"\"\"\n",
    "    Write the features back to a file\n",
    "\n",
    "    @param filepath the name of the file to write the features to\n",
    "    @param features dictionary of features (key: image id -> value: numpy ndarray of features)\n",
    "    \"\"\"\n",
    "    dump(features, open(filepath, \"wb\"))\n",
    "\n",
    "def load_features(filepath: str, img_ids: list[str] = []) -> dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Read the features from a file\n",
    "\n",
    "    @param filepath the name of the file to read the features from\n",
    "    @param img_ids list of image ids to get the features for. If empty list return all\n",
    "    @return features dictionary of features (key: image id -> value: numpy ndarray of features)\n",
    "    \"\"\"\n",
    "    features = load(open(filepath, \"rb\"))\n",
    "    if len(img_ids) > 0:\n",
    "        features = {img_id : features[img_id] for img_id in img_ids if img_id in features}\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_filename = \"features.p\"\n",
    "\n",
    "if os.path.isfile(features_filename):\n",
    "    features = load_features(features_filename, img_ids)\n",
    "    \n",
    "else:\n",
    "    features = extract_features(dataset_image_dirname)\n",
    "    \n",
    "    save_features(features_filename, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize descriptions\n",
    "\n",
    "def create_tokenizer(descriptions: dict[str, list[str]]) -> tf.keras.preprocessing.text.Tokenizer:\n",
    "    \"\"\"\n",
    "    @param descriptions a dictionary (key: image id -> value: array of image captions)\n",
    "    @return tokenizer tool that stores every word in the vocabuluary at an unique index\n",
    "    \"\"\"\n",
    "    descriptions_list = dict_to_list(descriptions)\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "    tokenizer.fit_on_texts(descriptions_list)\n",
    "    return tokenizer\n",
    "\n",
    "def save_tokenizer(filepath: str, tokenizer: tf.keras.preprocessing.text.Tokenizer) -> None:\n",
    "    \"\"\"\n",
    "    Write the tokenizer to a file\n",
    "\n",
    "    @param filepath the name of the file to write the tokens to\n",
    "    @param tokenizer tool that stores every word in the vocabuluary at an unique index\n",
    "    \"\"\"\n",
    "    dump(tokenizer, open(filepath, \"wb\"))\n",
    "\n",
    "def load_tokenizer(filepath: str) -> tf.keras.preprocessing.text.Tokenizer:\n",
    "    \"\"\"\n",
    "    Read the tokenizer from a file\n",
    "\n",
    "    @param filepath the name of the file to read the tokenizer from\n",
    "    @return tool that stores every word in the vocabuluary at an unique index\n",
    "    \"\"\"\n",
    "    tokenizer = load(open(filepath, \"rb\"))\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_filename = \"tokenizer.p\"\n",
    "\n",
    "if os.path.isfile(tokenizer_filename):\n",
    "    tokenizer = load_tokenizer(tokenizer_filename)\n",
    "\n",
    "else:\n",
    "    tokenizer = create_tokenizer(descriptions)\n",
    "    \n",
    "    save_tokenizer(tokenizer_filename, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find vocabulary features\n",
    "\n",
    "def get_vocab(descriptions: dict[str, list[str]]) -> set[str]:\n",
    "    \"\"\"\n",
    "    @param descriptions a dictionary (key: image id -> value: array of image captions)\n",
    "    @return set of all words used in captions\n",
    "    \"\"\"\n",
    "    vocab = set()\n",
    "    for img_id, captions in descriptions.items():\n",
    "        [vocab.update(words.split()) for words in captions]\n",
    "    return vocab\n",
    "\n",
    "def get_max_length(descriptions: dict[str, list[str]]) -> int:\n",
    "    \"\"\"\n",
    "    @param descriptions a dictionary (key: image id -> value: array of image captions)\n",
    "    @return the length of the longest caption\n",
    "    \"\"\"\n",
    "    descriptions_list = dict_to_list(descriptions)\n",
    "    max_length = max(len(caption.split()) for caption in descriptions_list)\n",
    "    return max_length\n",
    "\n",
    "def get_vocab_size(tokenizer: tf.keras.preprocessing.text.Tokenizer) -> int:\n",
    "    \"\"\"\n",
    "    @param tokenizer tool that stores every word in the vocabuluary at an unique index\n",
    "    @return the number of unique words given all captions\n",
    "    \"\"\"\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    return vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data generators to feed data into model\n",
    "\n",
    "def create_sequences(captions: list[str],\n",
    "                     feature: np.ndarray,\n",
    "                     tokenizer: tf.keras.preprocessing.text.Tokenizer,\n",
    "                     max_description_length: int,\n",
    "                     vocab_size: int) -> tuple[np.array, np.array, np.array]:\n",
    "    \"\"\"\n",
    "    Generate input/output sequences for a given image and caption set\n",
    "\n",
    "    @param captions list of captions of an image\n",
    "    @param feature feature array of an image\n",
    "    @param tokenizer tool that stores every word in the vocabuluary at an unique index\n",
    "    @param max_description_length the length of the longest caption\n",
    "    @param vocab_size the number of unique words given all captions\n",
    "    @return x1 2048 feature vector of image\n",
    "            x2 input text sequence for image\n",
    "            y predicted output text sequence for image\n",
    "    \"\"\"\n",
    "    x1 = []\n",
    "    x2 = []\n",
    "    y = []\n",
    "\n",
    "    for caption in captions:\n",
    "        # Encode sequence\n",
    "        seq = tokenizer.texts_to_sequences([caption])[0]\n",
    "\n",
    "        # Divide sequence into x, y pairs\n",
    "        for i in range(1, len(seq)):\n",
    "            in_seq = seq[:i]\n",
    "            out_seq = seq[i]\n",
    "            in_seq = tf.keras.preprocessing.sequence.pad_sequences([in_seq], maxlen=max_description_length)[0]\n",
    "            out_seq = tf.keras.utils.to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "\n",
    "            x1.append(feature)\n",
    "            x2.append(in_seq)\n",
    "            y.append(out_seq)\n",
    "    \n",
    "    return (np.array(x1), np.array(x2), np.array(y))\n",
    "\n",
    "def data_generator(descriptions: dict[str, list[str]],\n",
    "                   features: dict[str, np.ndarray],\n",
    "                   tokenizer: tf.keras.preprocessing.text.Tokenizer,\n",
    "                   max_description_length: int,\n",
    "                   vocab_size: int) -> list[list[np.array, np.array], np.array]:\n",
    "    \"\"\"\n",
    "    Yield input/output sequences for every image and caption set\n",
    "\n",
    "    @param descriptions a dictionary (key: image id -> value: array of image captions)\n",
    "    @param features dictionary of features (key: image id -> value: numpy ndarray of features)\n",
    "    @param tokenizer tool that stores every word in the vocabuluary at an unique index\n",
    "    @param max_description_length the length of the longest caption\n",
    "    @param vocab_size the number of unique words given all captions\n",
    "    @return input_image 2048 feature vector of image\n",
    "            input_sequence input text sequence for image\n",
    "            output_seq predicted output text sequence for image\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        for img_id, captions in descriptions.items():\n",
    "            feature = features[img_id][0]\n",
    "            input_img, input_seq, output_seq = create_sequences(captions, feature, tokenizer, max_description_length, vocab_size)\n",
    "            yield [[input_img, input_seq], output_seq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = get_vocab_size(tokenizer)\n",
    "max_description_length = get_max_length(descriptions)\n",
    "\n",
    "[input_img, input_seq], output_seq = next(data_generator(descriptions, features, tokenizer, max_description_length, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN-RNN model\n",
    "\n",
    "def define_model(max_description_length: int, vocab_size: int) -> tf.keras.models.Model:\n",
    "    \"\"\"\n",
    "    Define the CNN-RNN model\n",
    "\n",
    "    @param max_description_length the length of the longest caption\n",
    "    @param vocab_size the number of unique words given all captions\n",
    "    @return the CNN-RNN model\n",
    "    \"\"\"\n",
    "    # Features from CNN model compress from 2048 -> 256 nodes\n",
    "    inputs1 = tf.keras.layers.Input(shape=(2048,))\n",
    "    fe1 = tf.keras.layers.Dropout(0.5)(inputs1)\n",
    "    fe2 = tf.keras.layers.Dense(256, activation=\"relu\")(fe1)\n",
    "\n",
    "    # LSTM sequence model\n",
    "    inputs2 = tf.keras.layers.Input(shape=(max_description_length,))\n",
    "    se1 = tf.keras.layers.Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "    se2 =  tf.keras.layers.Dropout(0.5)(se1)\n",
    "    se3 = tf.keras.layers.LSTM(256)(se2)\n",
    "\n",
    "    # Merge both models\n",
    "    decoder1 = tf.keras.layers.add([fe2, se3])\n",
    "    decoder2 = tf.keras.layers.Dense(256, activation=\"relu\")(decoder1)\n",
    "    outputs = tf.keras.layers.Dense(vocab_size, activation=\"softmax\")(decoder2)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\")\n",
    "\n",
    "    print(model.summary())\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "print(\"Dataset:\", len(img_ids))\n",
    "print(\"Descriptions:\", len(descriptions))\n",
    "print(\"Features:\", len(features))\n",
    "print(\"Vocabulary Size:\", vocab_size)\n",
    "print(\"Description Length:\", max_description_length)\n",
    "\n",
    "model = define_model(max_description_length, vocab_size)\n",
    "epochs = 10\n",
    "steps = len(descriptions)\n",
    "\n",
    "model_dirname = \"models\"\n",
    "if not os.path.exists(model_dirname):\n",
    "    os.mkdir(model_dirname)\n",
    "\n",
    "for i in range(epochs):\n",
    "    model_name = model_dirname + \"/model_\" + str(i) + \".h5\"\n",
    "\n",
    "    generator = data_generator(descriptions, features, tokenizer, max_description_length, vocab_size)\n",
    "    model.fit(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
    "    model.save(model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.3 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0a6fbbda9a332fbeb69584f890bbc13262a5a64e4479e957e165bbafac6deac3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
